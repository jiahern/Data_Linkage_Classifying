{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMP20008_Project_2 \n",
    "# Created on 17th September 2019\n",
    "# Written by Chia-Wei Cheng, chiaweic (1025192), JiaHern Lee, jiahern (997562), KaiYuan Zheng, kzzhe (1024904)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import neighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import math,random\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "\n",
    "K_NN_5 = 5\n",
    "K_NN_10 = 10\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#similarity calculation function\n",
    "def similar(a,b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "#build a function to find matching pair by blocking method\n",
    "def find_pair (limit,thrhold,bsize):\n",
    "    i = 0\n",
    "    \n",
    "    #create an empty dataframe to store the matched pairs\n",
    "    pair_found = pd.DataFrame(columns = [\"idAmazon\",\"idGoogleBase\"])\n",
    "    \n",
    "    #each loop will generate a block for amazon and google, and add the matched pairs to pair_found\n",
    "    while i<limit:\n",
    "        #generate blocks with same blocking key\n",
    "        condition_a = ((amazon[\"price\"]>=i) & (amazon[\"price\"]<=i+bsize))\n",
    "        condition_g = ((google[\"price\"]>=i) & (google[\"price\"]<=i+bsize))\n",
    "        block_a = amazon[condition_a]\n",
    "        block_g = google[condition_g]\n",
    "        \n",
    "        #create two empty lists to store the index values\n",
    "        pair_index_a = []\n",
    "        pair_index_g = []\n",
    "        \n",
    "        #match the records from different blocks by attributes \"title\", \"name\"\n",
    "        for record1 in block_a[\"title\"]:\n",
    "            for record2 in block_g[\"name\"]:\n",
    "                score = similar(record1,record2)\n",
    "                if score > thrhold:\n",
    "                    pair_index_a.append(block_a[block_a[\"title\"]==record1].idAmazon.values[0])\n",
    "                    pair_index_g.append(block_g[block_g[\"name\"]==record2].id.values[0])\n",
    "        \n",
    "        #format the matched pairs into dataframe and append to the empty dataframe\n",
    "        pairs = pd.DataFrame({'idAmazon':pair_index_a,'idGoogleBase':pair_index_g})\n",
    "        pair_found = pair_found.append(pairs, ignore_index=True)\n",
    "        i+=bsize\n",
    "    return pair_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Matches: 130\n",
      "Correctly Matched: 96\n",
      "Recall: 0.7384615384615385\n",
      "Precision: 0.7218045112781954\n"
     ]
    }
   ],
   "source": [
    "# Naive data linkage\n",
    "\n",
    "#Read in datasets\n",
    "amazon_s = pd.read_csv(\"amazon_small.csv\")\n",
    "google_s = pd.read_csv('google_small.csv')\n",
    "gt_s = pd.read_csv('amazon_google_truth_small.csv')\n",
    "\n",
    "#create two empty lists to store the index values\n",
    "pair_index_A = []\n",
    "pair_index_G = []\n",
    "#link the records from different datasets by attributes 'title', 'name'\n",
    "for record1 in amazon_s[\"title\"]:\n",
    "    for record2 in google_s[\"name\"]:\n",
    "        score = similar(record1,record2)\n",
    "        #after several attempts, decide to use 0.65 as the threshold to obtain similar size of pairs with ground truth\n",
    "        if score > 0.54:\n",
    "            pair_index_A.append(amazon_s[amazon_s[\"title\"]==record1].idAmazon.values[0])\n",
    "            pair_index_G.append(google_s[google_s[\"name\"]==record2].idGoogleBase.values[0])\n",
    "\n",
    "pairs = pd.DataFrame({'idAmazon':pair_index_A,'idGoogleBase':pair_index_G})\n",
    "#merge the ground truth with our matched pairs to get true possitive pairs\n",
    "tp_pair = pd.merge(pairs, gt_s, how='inner', on=['idAmazon','idGoogleBase'])\n",
    "\n",
    "true_match = gt_s.shape[0]\n",
    "tp = tp_pair.shape[0]\n",
    "fn = gt_s.shape[0]-tp\n",
    "fp = pairs.shape[0]-tp\n",
    "\n",
    "#evaluate the perfomance of linkage by recall and precision\n",
    "recall = tp/(tp+fn)\n",
    "precision = tp/(tp+fp)\n",
    "\n",
    "print (\"True Matches:\",true_match)\n",
    "print (\"Correctly Matched:\",tp)\n",
    "print (\"Recall:\",recall)\n",
    "print (\"Precision:\",precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1 Na¨ıve data linkage without blocking\n",
    "\n",
    "#2. The linkage method we applied is based on the amazon_small(title) and google_small(name). This is due to there are no\n",
    "#    Null value in those attributes which may increase the accuracy of the similarity comparison. For similarity function,\n",
    "#    there are roughly three types of similarity function: Edit distance based, Token-based and Sequence-based(Pattern Search).   \n",
    "#    Among three of them, Sequence-based has the highest result. By applying Sequence-based similarity function, \n",
    "#    SequenceMatcher by difflib. The theory is based on Gestalt Pattern Matching, finding the longest common substring \n",
    "#    plus recursively the number of matching characters in the non-matching regions on both sides of the LCS. The more matching,\n",
    "#    the higher similarity score will be. The threshold(0.54) is set depends on the amount of rows of ground truth dataset. \n",
    "#    When threshold higher, the output row lesser.By using this function, it allows us to indicate the long strings in both \n",
    "#    of the attributes precisely. Overall, the performance's results are pretty presentable, recall(0.74) and precision(0.72).    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blocking method\n",
    "\n",
    "\n",
    "# It will roughly take you 3 mins\n",
    "\n",
    "#Read in different datasets\n",
    "google = pd.read_csv(\"google.csv\")\n",
    "amazon = pd.read_csv(\"amazon.csv\")\n",
    "gt = pd.read_csv(\"amazon_google_truth.csv\")\n",
    "\n",
    "#Data Cleaning google\n",
    "\n",
    "#convert price in google dataset, gbp pound sterling into aud \n",
    "for row in google.index:\n",
    "    if \"gbp\" in google.loc[row,\"price\"]:\n",
    "        price = google.loc[row,\"price\"]\n",
    "        google.loc[row,\"price\"] = 1.83*int(re.findall(\"\\d+\", price)[0])\n",
    "\n",
    "google['price'] = pd.to_numeric(google['price'], errors = 'coerce')\n",
    "\n",
    "#Data Cleaning amazon\n",
    "\n",
    "amazon.sort_values(\"price\")\n",
    "amazon['price'] = pd.to_numeric(amazon['price'], errors = 'coerce')\n",
    "\n",
    "#name the result as pair01\n",
    "#use 10e5 as limit since it is bigger than all price values in either amazon or google\n",
    "#after several attempts, decide to use 0.65 as the threshold to obtain similar size of pairs with ground truth\n",
    "pairs = find_pair(10e5,0.65,50)\n",
    "\n",
    "#merge the ground truth with our matched pairs to get true possitive pairs\n",
    "TP_pairs = pd.merge(pairs,gt,how=\"inner\",on=[\"idAmazon\",\"idGoogleBase\"])\n",
    "\n",
    "TP = TP_pairs.shape[0]\n",
    "FP = pairs.shape[0]-TP\n",
    "FN = gt.shape[0]-TP\n",
    "TN = amazon.shape[0]*google.shape[0]-TP\n",
    "n = TP + FP + FN + TN\n",
    "\n",
    "PC = TP/(TP+FN)\n",
    "RR = 1 - (TP+FP)/n\n",
    "\n",
    "print (\"True Matches:\",gt.shape[0])\n",
    "print (\"Correctly Matched:\",TP)\n",
    "print (\"Pair Completeness:\",PC)\n",
    "print (\"Reduction Ratio:\",RR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2 Blocking for efficient data linkage\n",
    "#For our blocking method, we choose to divide the blocks base on the price of the both datasets. \n",
    "#We allocate the rows according to different price values, and the ranges have the same gap. By applying this criteria, \n",
    "#we distribute the records into blocks in which the records have approxiamate price values. However, the pair completeness\n",
    "#is not very high and we get a high value of reduction ratio. We suppose there are two reasons for this result. First reason\n",
    "#is that the blocks we divide do not have the same size. Because it is not garanteed that same range of price will have same \n",
    "#amount of records. Secondly, we only take use of one attribute when calculating similarity scores, which will decrease \n",
    "#our matching accuracy. In addition to these aspects, the time consumed when blocking is applied is a third of the time \n",
    "#using naive data linkage. Therefore, the blocking method boosts the data linkage process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2 Pre-processing\n",
    "\n",
    "# Original dataset\n",
    "\n",
    "# Open yeast file\n",
    "original = pd.read_csv(\"all_yeast.csv\")\n",
    "\n",
    "# A list contains the name of each feature\n",
    "feature_list = original.keys()[1:9]\n",
    "\n",
    "med_mean = [\"median\", \"mean\"]\n",
    "result = pd.DataFrame(index = feature_list, columns = med_mean)\n",
    "result[\"median\"] = [original[i].median() for i in feature_list]\n",
    "result[\"mean\"] = [original[i].mean() for i in feature_list]\n",
    "\n",
    "result.style.format(\"{:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2 Pre-processing \n",
    "\n",
    "# Impute missing value (Mean imputation)\n",
    "\n",
    "# Open yeast file \n",
    "mean_yeast = pd.read_csv(\"all_yeast.csv\")\n",
    "\n",
    "# A list contains the name of each feature\n",
    "feature_list = mean_yeast.keys()[1:9]\n",
    "\n",
    "# A list contains the original value of mean of each feature\n",
    "feature_mean = [mean_yeast[i].mean() for i in feature_list]\n",
    "\n",
    "# Apply mean imputation\n",
    "for i in range(len(feature_list)):\n",
    "    mean_yeast[feature_list[i]] = mean_yeast[feature_list[i]].fillna(feature_mean[i])\n",
    "    \n",
    "# Construct a DataFrame for the result\n",
    "result_feature = [\"min\", \"median\", \"max\", \"mean\", \"standard deviation\"]\n",
    "result = pd.DataFrame(index = feature_list, columns = result_feature)\n",
    "result[\"min\"] = [mean_yeast[i].min() for i in feature_list]\n",
    "result[\"median\"] = [mean_yeast[i].median() for i in feature_list]\n",
    "result[\"max\"] = [mean_yeast[i].max() for i in feature_list]\n",
    "result[\"mean\"] = [mean_yeast[i].mean() for i in feature_list]\n",
    "result[\"standard deviation\"] = [mean_yeast[i].std() for i in feature_list]\n",
    "\n",
    "result.style.format(\"{:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2 Pre-processing\n",
    "\n",
    "# Impute missing value (median imputation)\n",
    "\n",
    "# Open yeast file\n",
    "median_yeast = pd.read_csv(\"all_yeast.csv\")\n",
    "\n",
    "# A list contains the name of each feature\n",
    "feature_list = median_yeast.keys()[1:9]\n",
    "\n",
    "# A list contains the original value of median of each feature\n",
    "feature_median = [mean_yeast[i].median() for i in feature_list]\n",
    "\n",
    "# Apply median imputation\n",
    "for i in range(len(feature_list)):\n",
    "    median_yeast[feature_list[i]] = median_yeast[feature_list[i]].fillna(feature_median[i])\n",
    "\n",
    "# Construct a DataFrame for the result\n",
    "result_feature = [\"min\", \"median\", \"max\", \"mean\", \"standard deviation\"]\n",
    "result = pd.DataFrame(index = feature_list, columns = result_feature)\n",
    "result[\"min\"] = [median_yeast[i].min() for i in feature_list]\n",
    "result[\"median\"] = [median_yeast[i].median() for i in feature_list]\n",
    "result[\"max\"] = [median_yeast[i].max() for i in feature_list]\n",
    "result[\"mean\"] = [median_yeast[i].mean() for i in feature_list]\n",
    "result[\"standard deviation\"] = [median_yeast[i].std() for i in feature_list]\n",
    "\n",
    "result.style.format(\"{:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2 Pre-processing\n",
    "\n",
    "# Evaluation (Mean imputation and median imputation)\n",
    "\n",
    "# Mean imputation does not change the mean value overall, while median imputation might affect the mean and the \n",
    "#  distribution of the dataset if the difference between the mean and median is huge.  Therefore, I suggest that \n",
    "#  it would be better to use mean imputation rather than median imputation if the dataset is widely spread.\n",
    "#  However, for all_yeast dataset, it can be seen that the median and the mean are rather close.  Thus, I \n",
    "#  believe that both methods are suitable for this datase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2 Pre-processing\n",
    "\n",
    "# Scale the features (Mean centering)\n",
    "\n",
    "# Create a feature list\n",
    "feature_list = mean_yeast.keys()[1:9]\n",
    "\n",
    "# Create a copy of median_yeast\n",
    "mean_centred_yeast = median_yeast.copy()\n",
    "\n",
    "# Substract mean from every value in every feature\n",
    "for feature in feature_list:\n",
    "    mean_centred_yeast[feature] = [float(i) - median_yeast[feature].mean() for i in mean_centred_yeast[feature]]\n",
    "    \n",
    "\n",
    "result_feature = [\"min\", \"median\", \"max\", \"mean\", \"standard deviation\"]\n",
    "result = pd.DataFrame(index = feature_list, columns = result_feature)\n",
    "result[\"min\"] = [mean_centred_yeast[i].min() for i in feature_list]\n",
    "result[\"median\"] = [mean_centred_yeast[i].median() for i in feature_list]\n",
    "result[\"max\"] = [mean_centred_yeast[i].max() for i in feature_list]\n",
    "result[\"mean\"] = [mean_centred_yeast[i].mean() for i in feature_list]\n",
    "result[\"standard deviation\"] = [mean_centred_yeast[i].std() for i in feature_list]\n",
    "\n",
    "result.style.format(\"{:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2 Pre-processing\n",
    "\n",
    "# Scale the features (Standardisation)\n",
    "\n",
    "# Create a feature list\n",
    "feature_list = mean_yeast.keys()[1:9]\n",
    "\n",
    "# Create a copy of median_yeast\n",
    "std_yeast = median_yeast.copy()\n",
    "\n",
    "# Substract mean and divide by standard deviation from every value in every feature\n",
    "for feature in feature_list:\n",
    "    std_yeast[feature] = [(float(i) - median_yeast[feature].mean())/median_yeast[feature].std() for i in std_yeast[feature]]\n",
    "        \n",
    "        \n",
    "result_feature = [\"min\", \"median\", \"max\", \"mean\", \"standard deviation\"]\n",
    "result = pd.DataFrame(index = feature_list, columns = result_feature)\n",
    "result[\"min\"] = [std_yeast[i].min() for i in feature_list]\n",
    "result[\"median\"] = [std_yeast[i].median() for i in feature_list]\n",
    "result[\"max\"] = [std_yeast[i].max() for i in feature_list]\n",
    "result[\"mean\"] = [std_yeast[i].mean() for i in feature_list]\n",
    "result[\"standard deviation\"] = [std_yeast[i].std() for i in feature_list]\n",
    "\n",
    "result.style.format(\"{:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2 Pre-processing\n",
    "\n",
    "# Evaluation (Scale the features)\n",
    "\n",
    "# Mean centering can be treated as the first step of standardisation as standardisation simply just divide every \n",
    "#  value by standard deviation after mean centering.  After performing Mean centering, the entire spectrum moves\n",
    "#  toward the 0 without changing the relationship between each point and will end up with 0 as the mean value.\n",
    "# After applying standardisation, the standard deviation of every feature become 1 which solves the issue of \n",
    "#  having data measured on different scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2 Comparing Classification Algorithms (Using mean centred yeast)\n",
    "\n",
    "# Create a feature list\n",
    "feature_list = mean_yeast.keys()[1:9]\n",
    "\n",
    "# Get the 8 features with its numerical data\n",
    "mean_centred_data = mean_centred_yeast[feature_list].astype(float)\n",
    "\n",
    "# Get the class labels\n",
    "classlabel = mean_centred_yeast['Class']\n",
    "\n",
    "# Randomly select 66% of the instances to be training and the rest to be testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(mean_centred_data,classlabel, train_size=0.66, test_size=0.34, random_state=42)\n",
    "\n",
    "# Normalise the data to have 0 mean and unit variance using the library functions.  \n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Appy K-NN algorithm\n",
    "knn_5 = neighbors.KNeighborsClassifier(n_neighbors = K_NN_5)\n",
    "knn_10 = neighbors.KNeighborsClassifier(n_neighbors = K_NN_10)\n",
    "knn_5.fit(X_train, y_train)\n",
    "knn_10.fit(X_train, y_train)\n",
    "\n",
    "# K = 5 and K = 10\n",
    "y_pred_5 = knn_5.predict(X_test)\n",
    "y_pred_10 = knn_10.predict(X_test)\n",
    "\n",
    "# Print the result\n",
    "print('K-NN (K = 5):', accuracy_score(y_test, y_pred_5))\n",
    "print('K-NN (K = 10):', accuracy_score(y_test, y_pred_10))\n",
    "\n",
    "# Construct the decision tree\n",
    "dt = DecisionTreeClassifier(criterion=\"entropy\",random_state=1, max_depth=3)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "y_pred=dt.predict(X_test)\n",
    "print('Decision tree:', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy score vs k_values (from 1 to 15)\n",
    "\n",
    "k_values = range(1, 15, 2)\n",
    "\n",
    "score_list = []\n",
    "for k in k_values:\n",
    "    knn = neighbors.KNeighborsClassifier(n_neighbors = k)\n",
    "    knn.fit(X_train, y_train) \n",
    "    y_pred = knn.predict(X_test)\n",
    "    score_list.append(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "\n",
    "plt.plot(k_values, score_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy score vs k_values (from 1 to 100)\n",
    "\n",
    "k_values = range(1, 100, 2)\n",
    "\n",
    "score_list = []\n",
    "for k in k_values:\n",
    "    knn = neighbors.KNeighborsClassifier(n_neighbors = k)\n",
    "    knn.fit(X_train, y_train) \n",
    "    y_pred = knn.predict(X_test)\n",
    "    score_list.append(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "\n",
    "plt.plot(k_values, score_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy score vs depth_values (from 1 to 100)\n",
    "\n",
    "depth_values = range(1, 100, 1)\n",
    "\n",
    "score_list = []\n",
    "for depth in depth_values:\n",
    "    dt = DecisionTreeClassifier(criterion=\"entropy\", random_state = 1, max_depth = depth)\n",
    "    dt.fit(X_train, y_train)\n",
    "    y_pred = dt.predict(X_test)\n",
    "    score_list.append(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "\n",
    "plt.plot(depth_values, score_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2 Comparing Classification Algorithms (Using median imputed yeast)\n",
    "\n",
    "# Create a feature list\n",
    "feature_list = median_yeast.keys()[1:9]\n",
    "\n",
    "# Get the 8 features with its numerical data\n",
    "median_data = median_yeast[feature_list].astype(float)\n",
    "\n",
    "# Get the class labels\n",
    "classlabel = median_yeast['Class']\n",
    "\n",
    "# Randomly select 66% of the instances to be training and the rest to be testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(median_data,classlabel, train_size = 0.66, test_size = 0.34, random_state = 42)\n",
    "\n",
    "# Normalise the data to have 0 mean and unit variance using the library functions.  \n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Appy K-NN algorithm\n",
    "knn_5 = neighbors.KNeighborsClassifier(n_neighbors = K_NN_5)\n",
    "knn_10 = neighbors.KNeighborsClassifier(n_neighbors = K_NN_10)\n",
    "knn_5.fit(X_train, y_train)\n",
    "knn_10.fit(X_train, y_train)\n",
    "\n",
    "# K = 5 and K = 10\n",
    "y_pred_5 = knn_5.predict(X_test)\n",
    "y_pred_10 = knn_10.predict(X_test)\n",
    "\n",
    "# Print the result\n",
    "print('K-NN (K = 5):', accuracy_score(y_test, y_pred_5))\n",
    "print('K-NN (K = 10):', accuracy_score(y_test, y_pred_10))\n",
    "\n",
    "# Construct the decision tree\n",
    "dt = DecisionTreeClassifier(criterion = \"entropy\",random_state = 1, max_depth = 3)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "y_pred = dt.predict(X_test)\n",
    "print('Decision tree:', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2 Comparing Classification Algorithms (Using median imputed yeast)\n",
    "\n",
    "# Create a feature list\n",
    "feature_list = median_yeast.keys()[1:9]\n",
    "\n",
    "# Get the 8 features with its numerical data\n",
    "median_data = median_yeast[feature_list].astype(float)\n",
    "\n",
    "# Get the class labels\n",
    "classlabel = median_yeast['Class']\n",
    "\n",
    "# Randomly select 66% of the instances to be training and the rest to be testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(median_data,classlabel, train_size = 0.66, test_size = 0.34, random_state = 42)\n",
    "\n",
    "# Normalise the data to have 0 mean and unit variance using the library functions.  \n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Appy K-NN algorithm\n",
    "knn_5 = neighbors.KNeighborsClassifier(n_neighbors = K_NN_5)\n",
    "knn_10 = neighbors.KNeighborsClassifier(n_neighbors = K_NN_10)\n",
    "knn_5.fit(X_train, y_train)\n",
    "knn_10.fit(X_train, y_train)\n",
    "\n",
    "# K = 5 and K = 10\n",
    "y_pred_5 = knn_5.predict(X_test)\n",
    "y_pred_10 = knn_10.predict(X_test)\n",
    "\n",
    "# Print the result\n",
    "print('K-NN (K = 5):', accuracy_score(y_test, y_pred_5))\n",
    "print('K-NN (K = 10):', accuracy_score(y_test, y_pred_10))\n",
    "\n",
    "# Construct the decision tree\n",
    "dt = DecisionTreeClassifier(criterion = \"entropy\",random_state = 1, max_depth = 3)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "y_pred = dt.predict(X_test)\n",
    "print('Decision tree:', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = range(1, 15, 2)\n",
    "\n",
    "score_list = []\n",
    "for k in k_values:\n",
    "    knn = neighbors.KNeighborsClassifier(n_neighbors = k)\n",
    "    knn.fit(X_train, y_train) \n",
    "    y_pred = knn.predict(X_test)\n",
    "    score_list.append(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "\n",
    "plt.plot(k_values, score_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy score vs k_values (from 1 to 100)\n",
    "\n",
    "k_values = range(1, 100, 2)\n",
    "\n",
    "score_list = []\n",
    "for k in k_values:\n",
    "    knn = neighbors.KNeighborsClassifier(n_neighbors = k)\n",
    "    knn.fit(X_train, y_train) \n",
    "    y_pred = knn.predict(X_test)\n",
    "    score_list.append(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "\n",
    "plt.plot(k_values, score_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy score vs depth_values (from 1 to 100)\n",
    "\n",
    "depth_values = range(1, 100, 1)\n",
    "\n",
    "score_list = []\n",
    "for depth in depth_values:\n",
    "    dt = DecisionTreeClassifier(criterion=\"entropy\", random_state = 1, max_depth = depth)\n",
    "    dt.fit(X_train, y_train)\n",
    "    y_pred = dt.predict(X_test)\n",
    "    score_list.append(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "\n",
    "plt.plot(depth_values, score_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part2 Comparing Classification Algorithms \n",
    "\n",
    "# Discussion\n",
    "\n",
    "# Both mean centred and median imputed dataset produce the same graph and accuracy scores.  It can be seen that\n",
    "#  K-NN K = 5 has the highest accuracy score, while decision tree algorithms produce the lowest score.  The highest\n",
    "#  accuracy that decision tree can achieve is around 0.7 when the depth is around 20, while the higest accuracy \n",
    "#  that K-NN can achieve is around 0.75 when K is around 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function for Feature Engineering part\n",
    "\n",
    "def feature_reduction(data, selection_list):\n",
    "    \n",
    "    ''' Take a 2D array which contains a set of N features (the original features plus generated features) \n",
    "        and return a 2D array after feature selection based on the selection_list '''\n",
    "    reduced_data = []\n",
    "    for i in data:\n",
    "        temp = []\n",
    "        for j in selection_list:\n",
    "            temp.append(i[j])\n",
    "        reduced_data.append(temp)\n",
    "    return reduced_data\n",
    "\n",
    "def feature_generator(data):\n",
    "    feature_list = data.keys()\n",
    "    for i in range(len(feature_list)-1):\n",
    "        for j in range(i, len(feature_list)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            data[\"{} * {}\".format(feature_list[i], feature_list[j])] = data[feature_list[i]] * data[feature_list[j]]\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def VAT(R):\n",
    "    \"\"\"\n",
    "\n",
    "    VAT algorithm adapted from matlab version:\n",
    "    http://www.ece.mtu.edu/~thavens/code/VAT.m\n",
    "\n",
    "    Args:\n",
    "        R (n*n double): Dissimilarity data input\n",
    "        R (n*D double): vector input (R is converted to sq. Euclidean distance)\n",
    "    Returns:\n",
    "        RV (n*n double): VAT-reordered dissimilarity data\n",
    "        C (n int): Connection indexes of MST in [0,n)\n",
    "        I (n int): Reordered indexes of R, the input data in [0,n)\n",
    "    \"\"\"\n",
    "        \n",
    "    R = np.array(R)\n",
    "    N, M = R.shape\n",
    "    if N != M:\n",
    "        R = squareform(pdist(R))\n",
    "        \n",
    "    J = list(range(0, N))\n",
    "    \n",
    "    y = np.max(R, axis=0)\n",
    "    i = np.argmax(R, axis=0)\n",
    "    j = np.argmax(y)\n",
    "    y = np.max(y)\n",
    "\n",
    "\n",
    "    I = i[j]\n",
    "    del J[I]\n",
    "\n",
    "    y = np.min(R[I,J], axis=0)\n",
    "    j = np.argmin(R[I,J], axis=0)\n",
    "    \n",
    "    I = [I, J[j]]\n",
    "    J = [e for e in J if e != J[j]]\n",
    "    \n",
    "    C = [1,1]\n",
    "    for r in range(2, N-1):   \n",
    "        y = np.min(R[I,:][:,J], axis=0)\n",
    "        i = np.argmin(R[I,:][:,J], axis=0)\n",
    "        j = np.argmin(y)        \n",
    "        y = np.min(y)      \n",
    "        I.extend([J[j]])\n",
    "        J = [e for e in J if e != J[j]]\n",
    "        C.extend([i[j]])\n",
    "    \n",
    "    y = np.min(R[I,:][:,J], axis=0)\n",
    "    i = np.argmin(R[I,:][:,J], axis=0)\n",
    "    \n",
    "    I.extend(J)\n",
    "    C.extend(i)\n",
    "    \n",
    "    RI = list(range(N))\n",
    "    for idx, val in enumerate(I):\n",
    "        RI[val] = idx\n",
    "\n",
    "    RV = R[I,:][:,I]\n",
    "    \n",
    "    return RV.tolist(), C, I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "NUM_FEATURES = 5\n",
    "\n",
    "# Part 2 Feature Engineering\n",
    "\n",
    "# Interaction term pairs (Using mean centred_yeast) (Multiplication Only)\n",
    "\n",
    "# Get the 8 features\n",
    "feature_list = mean_centred_yeast.keys()[1:9]\n",
    "\n",
    "# Generate interaction term pairs (All possible pairs on multiplication)\n",
    "mul_mean_centred_data = feature_generator(mean_centred_data.copy())\n",
    "\n",
    "    \n",
    "\n",
    "# Discretise all numerical values\n",
    "discrete_data = KBinsDiscretizer(n_bins = 2, encode = 'ordinal', strategy = 'uniform')\n",
    "discrete_data.fit(mul_mean_centred_data)\n",
    "discrete_mul_data = discrete_data.transform(mul_mean_centred_data)\n",
    "\n",
    "# Class label\n",
    "classlabel = mean_centred_yeast[\"Class\"]\n",
    "\n",
    "\n",
    "# Randomly select 66% of the instances to be training and the rest to be testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(discrete_mul_data,classlabel, train_size=0.66, test_size=0.34, random_state=42)\n",
    "\n",
    "\n",
    "# Normalise the data to have 0 mean and unit variance using the library functions.  \n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Calculate the mutual information between each column and the class label\n",
    "mutual_info_mul = mutual_info_classif(X_train, y_train, discrete_features = True)\n",
    "\n",
    "# Create a dataframe \n",
    "score = pd.DataFrame({\"feature\": mul_mean_centred_data.keys(),\"score\":mutual_info_mul}).sort_values(by = \"score\", ascending = False)\n",
    "selection_list = list(score.iloc[:NUM_FEATURES,:].index)\n",
    "\n",
    "\n",
    "reduced_X_train = feature_reduction(X_train, selection_list)\n",
    "reduced_X_test = feature_reduction(X_test, selection_list)\n",
    "\n",
    "\n",
    "# Appy K-NN algorithm\n",
    "knn_5 = neighbors.KNeighborsClassifier(n_neighbors = K_NN_5)\n",
    "knn_10 = neighbors.KNeighborsClassifier(n_neighbors = K_NN_10)\n",
    "knn_5.fit(reduced_X_train, y_train)\n",
    "knn_10.fit(reduced_X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred_5 = knn_5.predict(reduced_X_test)\n",
    "y_pred_10 = knn_10.predict(reduced_X_test)\n",
    "print('K-NN (K = 5):', accuracy_score(y_test, y_pred_5))\n",
    "print('K-NN (K = 10):', accuracy_score(y_test, y_pred_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2 Feature Engineering\n",
    "\n",
    "# Clustering labels (Using mean centred data)\n",
    "\n",
    "mean_centred_cluster = mean_centred_data.copy()\n",
    "\n",
    "clusters = KMeans(n_clusters = 5).fit(mean_centred_cluster)\n",
    "\n",
    "cluster_classlabel = clusters.labels_\n",
    "\n",
    "# Randomly select 66% of the instances to be training and the rest to be testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(mean_centred_cluster,cluster_classlabel, train_size=0.66, test_size=0.34, random_state=42)\n",
    "\n",
    "# Normalise the data to have 0 mean and unit variance using the library functions.  \n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train=scaler.transform(X_train)\n",
    "X_test=scaler.transform(X_test)\n",
    "\n",
    "# Appy K-NN algorithm\n",
    "knn_5 = neighbors.KNeighborsClassifier(n_neighbors = K_NN_5)\n",
    "knn_10 = neighbors.KNeighborsClassifier(n_neighbors = K_NN_10)\n",
    "knn_5.fit(reduced_X_train, y_train)\n",
    "knn_10.fit(reduced_X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred_5 = knn_5.predict(reduced_X_test)\n",
    "y_pred_10 = knn_10.predict(reduced_X_test)\n",
    "print('K-NN (K = 5):', accuracy_score(y_test, y_pred_5))\n",
    "print('K-NN (K = 10):', accuracy_score(y_test, y_pred_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2 Feature Engineering \n",
    "\n",
    "# Feature generation and selection with cluster labels (Using mean centred data)\n",
    "mul_cluster_data = feature_generator(mean_centred_data.copy())\n",
    "\n",
    "\n",
    "clusters = KMeans(n_clusters = 5).fit(mean_centred_cluster)\n",
    "\n",
    "cluster_classlabel = clusters.labels_\n",
    "\n",
    "# Randomly select 66% of the instances to be training and the rest to be testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(mul_cluster_data,cluster_classlabel, train_size=0.66, test_size=0.34, random_state=42)\n",
    "\n",
    "# Normalise the data to have 0 mean and unit variance using the library functions.  \n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train=scaler.transform(X_train)\n",
    "X_test=scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Calculate the mutual information between each column and the class label\n",
    "mutual_info_mul = mutual_info_classif(X_train, y_train, discrete_features = True)\n",
    "\n",
    "# Create a dataframe \n",
    "score = pd.DataFrame({\"feature\": mul_mean_centred_data.keys(),\"score\":mutual_info_mul}).sort_values(by = \"score\", ascending = False)\n",
    "selection_list = list(score.iloc[:NUM_FEATURES,:].index)\n",
    "\n",
    "\n",
    "reduced_X_train = feature_reduction(X_train, selection_list)\n",
    "reduced_X_test = feature_reduction(X_test, selection_list)\n",
    "\n",
    "\n",
    "# Appy K-NN algorithm\n",
    "knn_5 = neighbors.KNeighborsClassifier(n_neighbors = K_NN_5)\n",
    "knn_10 = neighbors.KNeighborsClassifier(n_neighbors = K_NN_10)\n",
    "knn_5.fit(reduced_X_train, y_train)\n",
    "knn_10.fit(reduced_X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred_5 = knn_5.predict(reduced_X_test)\n",
    "y_pred_10 = knn_10.predict(reduced_X_test)\n",
    "print('K-NN (K = 5):', accuracy_score(y_test, y_pred_5))\n",
    "print('K-NN (K = 10):', accuracy_score(y_test, y_pred_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RV, C, I = VAT(mean_centred_data)\n",
    "x=sns.heatmap(RV,cmap='viridis',xticklabels=False,yticklabels=False)\n",
    "x.set(xlabel='Objects', ylabel='Objects')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2 Feature Engineering Discussion\n",
    "\n",
    "# Since both mean centred and median imputed dataset produce rather similar results, we only present the mean\n",
    "#  centred part.\n",
    "\n",
    "# Interaction term pairs\n",
    "\n",
    "# Since f1/f2 might have the situations that f2 = 0, We only consider all possible pairs of multiplication.\n",
    "# I choose 5 as the number of features selection as the accuracy score nearly remains the same when the value \n",
    "#  is larger than 5.  Eventhough 1 produce a similar score, I believe that having only the highest correlated \n",
    "#  column being selected might result in a overfitting situation.  Besides, Every column has rather low \n",
    "#  mutual information with the class, therefore, having only one column to predict the result might not be a \n",
    "#  good prediction.\n",
    "\n",
    "# Feature generation and selection\n",
    "\n",
    "# Applying feature generation and selection do not boost the accuracy score even if we change \n",
    "#  the number of bins during the discretisation stage.  This is might due to the fact that most of the points \n",
    "#  are very close to each other.  Thus, generating more points by multiplication or division might not be \n",
    "#  helpful for the prediction\n",
    "\n",
    "\n",
    "# Feature generation with clustering labels \n",
    "\n",
    "# The accuracy score is 1 when k = 1 and the score starts decreasing when k is increased\n",
    "# By plotting the heap map, it can be seen that there is only a huge cluster.  Since it is meaningless to \n",
    "#  have k = 1, we claim that using K-means algorithm is not a suitable solution for this case.\n",
    "# As a result, using resulting cluster labels as the values for the class does not boost the accuracy score.\n",
    "# What is more, it does not provide any useful information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
